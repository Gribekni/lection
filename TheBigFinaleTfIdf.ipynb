{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8dc60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas openpyxl deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "df = pd.read_excel(\"/home/roman/Desktop/Роман Грибов - dialog_talk_agent.xlsx\")\n",
    "\n",
    "df_valid = df.dropna(subset=[\"Text Response\"]).copy()\n",
    "print(f\"Строк с ответами: {len(df_valid)}\")\n",
    "\n",
    "translator = GoogleTranslator(source='en', target='ru')\n",
    "\n",
    "def safe_translate(text):\n",
    "    return translator.translate(str(text).strip())\n",
    "\n",
    "print(\"Перевод Context...\")\n",
    "df_valid[\"Context_ru\"] = df_valid[\"Context\"].astype(str).apply(safe_translate)\n",
    "\n",
    "print(\"Перевод Text Response...\")\n",
    "df_valid[\"Text Response_ru\"] = df_valid[\"Text Response\"].astype(str).apply(safe_translate)\n",
    "\n",
    "output_file = \"dialog_talk_agent_ru_clean.xlsx\"\n",
    "df_valid.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"\\nСохранено: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39a828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лемматизация Context_ru...\n",
      "✅ Лемматизация завершена!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "df = pd.read_excel(\"dialog_talk_agent_ru_clean.xlsx\")\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "def lemmatize_ru(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    doc = nlp(text.lower())\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_space:\n",
    "            continue\n",
    "        lemmas.append(token.lemma_)\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "print(\"Лемматизация Context_ru...\")\n",
    "df[\"Lemmas_ru\"] = df[\"Context_ru\"].apply(lemmatize_ru)\n",
    "\n",
    "df.to_excel(\"dialog_talk_agent_ru_lemmatized.xlsx\", index=False)\n",
    "print(\"✅ Лемматизация завершена!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb313c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ваше восприятие. Моя реальность.', np.float64(0.7929524434582053), 'ты чудак')\n",
      "('О, нет. В чем дело?', np.float64(0.8240422779108789), 'Мне грустно')\n",
      "('Виртуальный мир — моя игровая площадка. Я всегда на расстоянии нескольких кликов мыши.', np.float64(0.7856314550015997), 'твой дом')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "\n",
    "df = pd.read_excel(\"dialog_talk_agent_ru_lemmatized.xlsx\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=False,   \n",
    "    stop_words=None   \n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"Lemmas_ru\"])\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "def lemmatize_query(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    doc = nlp(text.lower())\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def get_response(user_input: str) -> str:\n",
    "    query_lem = lemmatize_query(user_input)\n",
    "    \n",
    "    query_vec = vectorizer.transform([query_lem])\n",
    "    \n",
    "    sims = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    \n",
    "    best_idx = sims.argmax()\n",
    "    best_score = sims[best_idx]\n",
    "    \n",
    "    if best_score < 0.1:\n",
    "        return \"Что чёрт возьми ты несёшь?\"\n",
    "    \n",
    "    return df.iloc[best_idx][\"Text Response_ru\"], best_score, df.iloc[best_idx][\"Context_ru\"]\n",
    "\n",
    "print(get_response(\"Ты такой чудак\"))\n",
    "print(get_response(\"грустно\"))\n",
    "print(get_response(\"Где твой дом?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9491825",
   "metadata": {},
   "source": [
    "Что ж, решил перевести тот английский датасет из гугл класса, работает с ооочень переменным успехом, тут скорее трудности перевода"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
